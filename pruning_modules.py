"""
Token Pruning æ ¸å¿ƒæ¨¡å—
å®ç°å®Œæ•´çš„ Token Pruning åŠŸèƒ½ï¼ŒåŒ…æ‹¬ä¿®æ”¹çš„ Transformer Block å’Œ Attention Processor
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any
from diffusers.models.attention_processor import Attention
from diffusers.models.attention_dispatch import dispatch_attention_fn


class TokenPruningCache:
    """
    ç®¡ç† Token Pruning çš„ç¼“å­˜
    
    ç¼“å­˜ç­–ç•¥:
    - æ­¥éª¤ 0 (ç¬¬1æ­¥): å®Œæ•´è®¡ç®—ï¼Œç¼“å­˜æ‰€æœ‰å±‚çš„ image tokens hidden states
    - æ­¥éª¤ 1 (ç¬¬2æ­¥): ä½¿ç”¨æ­¥éª¤ 0 çš„ç¼“å­˜
    - æ­¥éª¤ 2 (ç¬¬3æ­¥): å®Œæ•´è®¡ç®—ï¼Œç¼“å­˜æ‰€æœ‰å±‚çš„ image tokens hidden states
    - æ­¥éª¤ 3 (ç¬¬4æ­¥): ä½¿ç”¨æ­¥éª¤ 2 çš„ç¼“å­˜
    """
    
    def __init__(self):
        self.enabled = False
        self.current_step = 0  # å½“å‰å»å™ªæ­¥éª¤ (0-3)
        self.denoise_token_length = None  # å»å™ª tokens æ•°é‡
        self.total_token_length = None    # æ€» tokens æ•°é‡
        
        # æ¯å±‚çš„ç¼“å­˜: {layer_idx: {step: hidden_states}}
        # ä¾‹å¦‚: layer_caches[0][0] = ç¬¬0å±‚åœ¨æ­¥éª¤0çš„ image tokens hidden states
        self.layer_caches = {}  # {layer_idx: {0: tensor, 2: tensor}}
        
        # âš¡ é¢„åˆ†é…çš„ bufferï¼ˆé¿å…é¢‘ç¹çš„ GPU å†…å­˜åˆ†é…ï¼‰
        self._preallocated_buffers = {}
        self._buffers_initialized = False
        
        # ğŸ”¬ æ€§èƒ½è°ƒè¯•ï¼šè®°å½•ç¼“å­˜æ“ä½œæ—¶é—´
        self.debug_timing = False  # âš ï¸ é»˜è®¤å…³é—­ï¼Œé¿å…åŒæ­¥å¼€é”€
        self.cache_write_time = 0.0  # ç´¯ç§¯ç¼“å­˜å†™å…¥æ—¶é—´
        self.cache_read_time = 0.0   # ç´¯ç§¯ç¼“å­˜è¯»å–æ—¶é—´
        self.num_cache_writes = 0
        self.num_cache_reads = 0
        
        # ğŸ”¬ æ›´ç»†è‡´çš„è®¡æ—¶
        self.clone_time = 0.0  # clone() æ“ä½œæ—¶é—´
        self.copy_time = 0.0   # copy_() æ“ä½œæ—¶é—´
        self.num_clones = 0
        self.num_copies = 0
        
    def should_prune_current_step(self) -> bool:
        """åˆ¤æ–­å½“å‰æ­¥éª¤æ˜¯å¦éœ€è¦ prune"""
        if not self.enabled:
            return False
        # æ­¥éª¤ 1 å’Œ 3 (ç´¢å¼• 1, 3) éœ€è¦ä½¿ç”¨ç¼“å­˜
        return self.current_step in [1, 3]
    
    def get_cache_step_idx(self) -> Optional[int]:
        """è·å–åº”è¯¥ä½¿ç”¨å“ªä¸€æ­¥çš„ç¼“å­˜"""
        if self.current_step == 1:
            return 0  # æ­¥éª¤ 2 ä½¿ç”¨æ­¥éª¤ 1 çš„ç¼“å­˜
        elif self.current_step == 3:
            return 2  # æ­¥éª¤ 4 ä½¿ç”¨æ­¥éª¤ 3 çš„ç¼“å­˜
        return None
    
    def should_cache_current_step(self) -> bool:
        """åˆ¤æ–­å½“å‰æ­¥éª¤æ˜¯å¦éœ€è¦ç¼“å­˜"""
        if not self.enabled:
            return False
        # æ­¥éª¤ 0 å’Œ 2 (æ­¥éª¤ 1 å’Œ 3) éœ€è¦ç¼“å­˜
        return self.current_step in [0, 2]
    
    def _init_buffers_if_needed(self, layer_idx: int, image_k: torch.Tensor, image_v: torch.Tensor, image_hidden: torch.Tensor = None):
        """âš¡ é¢„åˆ†é…ç¼“å­˜ bufferï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼‰"""
        if layer_idx in self._preallocated_buffers:
            return
        
        # ä¸ºæ¯ä¸€å±‚é¢„åˆ†é… 2 ä¸ªæ­¥éª¤çš„ç¼“å­˜ï¼ˆæ­¥éª¤ 0 å’Œ 2ï¼‰
        self._preallocated_buffers[layer_idx] = {
            0: {
                'k': torch.empty_like(image_k),
                'v': torch.empty_like(image_v),
                'hidden': torch.empty_like(image_hidden) if image_hidden is not None else None,
            },
            2: {
                'k': torch.empty_like(image_k),
                'v': torch.empty_like(image_v),
                'hidden': torch.empty_like(image_hidden) if image_hidden is not None else None,
            }
        }
    
    def cache_layer_kv(self, layer_idx: int, image_k: torch.Tensor, image_v: torch.Tensor, image_hidden: torch.Tensor = None):
        """âš¡ ç¼“å­˜æŸä¸€å±‚çš„ image tokens K, Vï¼ˆä½¿ç”¨é¢„åˆ†é…çš„ bufferï¼Œé¿å… cudaMallocï¼‰"""
        if not self.should_cache_current_step():
            return
        
        # ğŸ”¬ å¼€å§‹è®¡æ—¶
        if self.debug_timing:
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            start_event.record()
        
        # âš¡ ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆå§‹åŒ– buffer
        self._init_buffers_if_needed(layer_idx, image_k, image_v, image_hidden)
        
        if layer_idx not in self.layer_caches:
            self.layer_caches[layer_idx] = {}
        
        # âš¡ ä½¿ç”¨é¢„åˆ†é…çš„ bufferï¼Œé€šè¿‡ copy_() è¿›è¡Œ in-place æ‹·è´
        # é¿å… clone() è§¦å‘æ–°çš„å†…å­˜åˆ†é…
        buffer = self._preallocated_buffers[layer_idx][self.current_step]
        
        # ğŸ”¬ è®¡æ—¶ copy æ“ä½œ
        if self.debug_timing:
            copy_start = torch.cuda.Event(enable_timing=True)
            copy_end = torch.cuda.Event(enable_timing=True)
            copy_start.record()
        
        buffer['k'].copy_(image_k)
        buffer['v'].copy_(image_v)
        if image_hidden is not None and buffer['hidden'] is not None:
            buffer['hidden'].copy_(image_hidden)
        
        # ğŸ”¬ ç»“æŸè®¡æ—¶
        if self.debug_timing:
            copy_end.record()
            torch.cuda.synchronize()
            copy_elapsed = copy_start.elapsed_time(copy_end) / 1000.0
            self.copy_time += copy_elapsed
            self.num_copies += 1
        
        # å­˜å‚¨ buffer çš„å¼•ç”¨
        self.layer_caches[layer_idx][self.current_step] = buffer
        
        # ğŸ”¬ ç»“æŸè®¡æ—¶
        if self.debug_timing:
            end_event.record()
            torch.cuda.synchronize()
            elapsed = start_event.elapsed_time(end_event) / 1000.0  # è½¬æ¢ä¸ºç§’
            self.cache_write_time += elapsed
            self.num_cache_writes += 1
    
    def update_layer_hidden(self, layer_idx: int, image_hidden: torch.Tensor):
        """âš¡ æ›´æ–°æŸä¸€å±‚ç¼“å­˜ä¸­çš„ hidden statesï¼ˆä½¿ç”¨é¢„åˆ†é…çš„ bufferï¼‰"""
        if not self.should_cache_current_step():
            return
        
        if layer_idx in self.layer_caches and self.current_step in self.layer_caches[layer_idx]:
            buffer = self.layer_caches[layer_idx][self.current_step]
            
            # âš¡ å¦‚æœ buffer çš„ hidden è¿˜æ²¡åˆå§‹åŒ–ï¼ˆç¬¬ä¸€æ¬¡é‡åˆ° hiddenï¼‰
            if buffer['hidden'] is None:
                buffer['hidden'] = torch.empty_like(image_hidden)
            
            # âš¡ ä½¿ç”¨ copy_() è¿›è¡Œ in-place æ‹·è´
            buffer['hidden'].copy_(image_hidden)
    
    def get_cached_layer_kv(self, layer_idx: int):
        """è·å–æŸä¸€å±‚çš„ç¼“å­˜ image tokens K, V å’Œ hidden statesï¼ˆå…¨éƒ¨åœ¨ GPU ä¸Šï¼‰"""
        if not self.should_prune_current_step():
            raise RuntimeError("åœ¨é Pruning æ­¥éª¤è°ƒç”¨ get_cached_layer_kvï¼")
        
        # ğŸ”¬ å¼€å§‹è®¡æ—¶
        if self.debug_timing:
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            start_event.record()
        
        cache_step = self.get_cache_step_idx()
        if cache_step is None:
            raise RuntimeError(f"æ— æ³•ç¡®å®šç¼“å­˜æ­¥éª¤ï¼current_step={self.current_step}")
        
        if layer_idx not in self.layer_caches:
            raise RuntimeError(f"Layer {layer_idx} æ²¡æœ‰ä»»ä½•ç¼“å­˜ï¼Available layers: {list(self.layer_caches.keys())}")
        
        if cache_step not in self.layer_caches[layer_idx]:
            raise RuntimeError(
                f"Layer {layer_idx} ç¼ºå°‘æ­¥éª¤ {cache_step} çš„ç¼“å­˜ï¼"
                f"Available steps: {list(self.layer_caches[layer_idx].keys())}"
            )
        
        cache_dict = self.layer_caches[layer_idx][cache_step]
        result = cache_dict['k'], cache_dict['v'], cache_dict['hidden']
        
        # ğŸ”¬ ç»“æŸè®¡æ—¶
        if self.debug_timing:
            end_event.record()
            torch.cuda.synchronize()
            elapsed = start_event.elapsed_time(end_event) / 1000.0
            self.cache_read_time += elapsed
            self.num_cache_reads += 1
        
        return result
    
    def clear_caches(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼ˆä½†ä¿ç•™é¢„åˆ†é…çš„ bufferï¼‰"""
        self.layer_caches = {}
        # âš¡ æ³¨æ„ï¼šä¸æ¸…ç©º _preallocated_buffersï¼Œå¤ç”¨å·²åˆ†é…çš„å†…å­˜
    
    def print_timing_stats(self):
        """ğŸ”¬ æ‰“å°ç¼“å­˜æ“ä½œçš„æ—¶é—´ç»Ÿè®¡"""
        if not self.debug_timing:
            return
        
        print("\n" + "=" * 70)
        print("ğŸ”¬ ç¼“å­˜æ“ä½œæ€§èƒ½ç»Ÿè®¡ï¼ˆè¯¦ç»†ï¼‰")
        print("=" * 70)
        
        print(f"1ï¸âƒ£ Clone æ“ä½œ:")
        print(f"  æ¬¡æ•°: {self.num_clones}")
        print(f"  æ—¶é—´: {self.clone_time:.4f}s")
        if self.num_clones > 0:
            print(f"  å¹³å‡: {self.clone_time/self.num_clones*1000:.2f}ms/æ¬¡")
        
        print(f"\n2ï¸âƒ£ Copy æ“ä½œ:")
        print(f"  æ¬¡æ•°: {self.num_copies}")
        print(f"  æ—¶é—´: {self.copy_time:.4f}s")
        if self.num_copies > 0:
            print(f"  å¹³å‡: {self.copy_time/self.num_copies*1000:.2f}ms/æ¬¡")
        
        print(f"\n3ï¸âƒ£ ç¼“å­˜å†™å…¥ï¼ˆclone + copyï¼‰:")
        print(f"  æ€»æ¬¡æ•°: {self.num_cache_writes}")
        print(f"  æ€»æ—¶é—´: {self.cache_write_time:.4f}s")
        if self.num_cache_writes > 0:
            print(f"  å¹³å‡: {self.cache_write_time/self.num_cache_writes*1000:.2f}ms/æ¬¡")
        
        print(f"\n4ï¸âƒ£ ç¼“å­˜è¯»å–:")
        print(f"  æ€»æ¬¡æ•°: {self.num_cache_reads}")
        print(f"  æ€»æ—¶é—´: {self.cache_read_time:.4f}s")
        if self.num_cache_reads > 0:
            print(f"  å¹³å‡: {self.cache_read_time/self.num_cache_reads*1000:.2f}ms/æ¬¡")
        
        print(f"\n" + "-" * 70)
        print(f"ğŸ“Š æ€»ç¼“å­˜å¼€é”€: {self.cache_write_time + self.cache_read_time:.4f}s")
        print(f"   - Clone è´¡çŒ®: {self.clone_time:.4f}s ({self.clone_time/(self.cache_write_time+self.cache_read_time+0.0001)*100:.1f}%)")
        print(f"   - Copy è´¡çŒ®: {self.copy_time:.4f}s ({self.copy_time/(self.cache_write_time+self.cache_read_time+0.0001)*100:.1f}%)")
        print("=" * 70)
    
    def reset_timing_stats(self):
        """é‡ç½®æ—¶é—´ç»Ÿè®¡"""
        self.cache_write_time = 0.0
        self.cache_read_time = 0.0
        self.num_cache_writes = 0
        self.num_cache_reads = 0
    
    def get_image_token_slice(self):
        """è·å– image tokens çš„åˆ‡ç‰‡èŒƒå›´"""
        if self.denoise_token_length is None:
            return None
        return slice(self.denoise_token_length, self.total_token_length)


# å…¨å±€ pruning cache å®ä¾‹
global_pruning_cache = TokenPruningCache()


# âš¡ å…¨å±€ RoPE å‡½æ•°ï¼ˆé¿å…åœ¨æ¯å±‚é‡å¤å®šä¹‰ï¼‰
def apply_rotary_emb_qwen(x, freqs, use_real=True):
    """ç®€åŒ–çš„ RoPE å®ç°"""
    if use_real:
        cos, sin = freqs
        cos = cos[None, None].to(x.device)
        sin = sin[None, None].to(x.device)
        x_real, x_imag = x.reshape(*x.shape[:-1], -1, 2).unbind(-1)
        x_rotated = torch.stack([-x_imag, x_real], dim=-1).flatten(3)
        return (x.float() * cos + x_rotated.float() * sin).to(x.dtype)
    else:
        x_rotated = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))
        freqs = freqs.unsqueeze(1)
        x_out = torch.view_as_real(x_rotated * freqs).flatten(3)
        return x_out.type_as(x)


class PrunableQwenDoubleStreamAttnProcessor:
    """
    æ”¯æŒ Token Pruning çš„åŒæµæ³¨æ„åŠ›å¤„ç†å™¨
    
    Pruning ç­–ç•¥:
    - Image tokens ä¸è®¡ç®— Qï¼ˆä¸éœ€è¦ä¸»åŠ¨æŸ¥è¯¢ï¼‰
    - Image tokens ä½¿ç”¨ç¼“å­˜çš„ hidden states è®¡ç®— K, Vï¼ˆæä¾›è¢«æŸ¥è¯¢çš„ä¿¡æ¯ï¼‰
    - å»å™ª tokens æ­£å¸¸è®¡ç®— Q, K, V
    """
    
    def __init__(self):
        if not hasattr(F, "scaled_dot_product_attention"):
            raise ImportError("éœ€è¦ PyTorch 2.0+")
        self._attention_backend = None
        self._parallel_config = None
    
    def __call__(
        self,
        attn: Attention,
        hidden_states: torch.FloatTensor,
        encoder_hidden_states: torch.FloatTensor = None,
        encoder_hidden_states_mask: torch.FloatTensor = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        image_rotary_emb: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:
        """
        å¸¦ Token Pruning çš„æ³¨æ„åŠ›è®¡ç®—
        """
        if encoder_hidden_states is None:
            raise ValueError("éœ€è¦ encoder_hidden_states (text stream)")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ pruning
        should_prune = global_pruning_cache.should_prune_current_step()
        should_cache = global_pruning_cache.should_cache_current_step()
        L_denoise = global_pruning_cache.denoise_token_length
        
        seq_txt = encoder_hidden_states.shape[1]
        
        # ===== è·å–ç¼“å­˜çš„ K, Vï¼ˆå¦‚æœæ˜¯ pruning æ­¥éª¤ï¼‰=====
        if should_prune and L_denoise is not None:
            # âš ï¸ Pruning æ¨¡å¼ï¼šå¿…é¡»æœ‰ç¼“å­˜ï¼Œå¦åˆ™æŠ¥é”™
            layer_idx = getattr(attn, '_layer_idx', None)
            if layer_idx is None:
                raise RuntimeError(f"Pruning æ¨¡å¼ä¸‹ attn ç¼ºå°‘ _layer_idx å±æ€§ï¼")
            
            cached_image_k, cached_image_v, cached_image_hidden = global_pruning_cache.get_cached_layer_kv(layer_idx)
            
            if cached_image_k is None or cached_image_v is None:
                raise RuntimeError(
                    f"Pruning æ¨¡å¼ä¸‹ç¼ºå°‘ç¼“å­˜ï¼\n"
                    f"  Layer: {layer_idx}\n"
                    f"  Step: {global_pruning_cache.current_step}\n"
                    f"  Expected cache step: {global_pruning_cache.get_cache_step_idx()}\n"
                    f"  Available caches: {list(global_pruning_cache.layer_caches.get(layer_idx, {}).keys())}"
                )
        
        # ===== è®¡ç®— QKV =====
        if should_prune:
            # âš¡ Pruning æ¨¡å¼ï¼šä½¿ç”¨ç¼“å­˜çš„ K, Vï¼ˆå·²ç»è¿‡ reshape/norm/RoPEï¼‰ï¼Œä¸é‡æ–°è®¡ç®—ï¼
            denoise_hidden = hidden_states[:, :L_denoise]
            
            # å»å™ª tokens: æŠ•å½±
            denoise_query = attn.to_q(denoise_hidden)
            denoise_key = attn.to_k(denoise_hidden)
            denoise_value = attn.to_v(denoise_hidden)
            
            # å»å™ª tokens: reshape + norm
            denoise_query = denoise_query.unflatten(-1, (attn.heads, -1))
            denoise_key = denoise_key.unflatten(-1, (attn.heads, -1))
            denoise_value = denoise_value.unflatten(-1, (attn.heads, -1))
            
            if attn.norm_q is not None:
                denoise_query = attn.norm_q(denoise_query)
            if attn.norm_k is not None:
                denoise_key = attn.norm_k(denoise_key)
            
            # å»å™ª tokens: RoPE
            if image_rotary_emb is not None:
                img_freqs, _ = image_rotary_emb
                seq_len_denoise = denoise_query.shape[1]
                img_freqs_for_denoise = img_freqs[:seq_len_denoise]
                denoise_query = apply_rotary_emb_qwen(denoise_query, img_freqs_for_denoise, use_real=False)
                denoise_key = apply_rotary_emb_qwen(denoise_key, img_freqs_for_denoise, use_real=False)
            
            # å›¾åƒ tokens: âš¡âš¡âš¡ ç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼ˆå·²ç»è¿‡å®Œæ•´å¤„ç†ï¼‰ï¼Œå®Œå…¨è·³è¿‡è®¡ç®—ï¼
            # åˆå¹¶ Q, K, V
            img_query = denoise_query  # åªæœ‰å»å™ªéƒ¨åˆ†æœ‰ Q
            img_key = torch.cat([denoise_key, cached_image_k], dim=1)     # âš¡ ä½¿ç”¨ç¼“å­˜ï¼ˆå·²å¤„ç†ï¼‰
            img_value = torch.cat([denoise_value, cached_image_v], dim=1)  # âš¡ ä½¿ç”¨ç¼“å­˜ï¼ˆå·²å¤„ç†ï¼‰
            
            # âš¡âš¡âš¡ è·³è¿‡åç»­çš„ reshape/norm/RoPEï¼ˆå·²ç»åœ¨ç¼“å­˜ä¸­å®Œæˆï¼‰
            skip_transform = True
            
        else:
            skip_transform = False
            # æ­£å¸¸æ¨¡å¼ï¼šå®Œæ•´è®¡ç®—
            img_query = attn.to_q(hidden_states)
            img_key = attn.to_k(hidden_states)
            img_value = attn.to_v(hidden_states)
        
        # æ–‡æœ¬æµï¼šå§‹ç»ˆæ­£å¸¸è®¡ç®—
        txt_query = attn.add_q_proj(encoder_hidden_states)
        txt_key = attn.add_k_proj(encoder_hidden_states)
        txt_value = attn.add_v_proj(encoder_hidden_states)
        
        # ===== Reshape for multi-head attention =====
        # âš¡ å¦‚æœä½¿ç”¨äº†ç¼“å­˜ï¼Œimg éƒ¨åˆ†å·²ç» reshape è¿‡äº†
        if not skip_transform:
            img_query = img_query.unflatten(-1, (attn.heads, -1))
            img_key = img_key.unflatten(-1, (attn.heads, -1))
            img_value = img_value.unflatten(-1, (attn.heads, -1))
        
        txt_query = txt_query.unflatten(-1, (attn.heads, -1))
        txt_key = txt_key.unflatten(-1, (attn.heads, -1))
        txt_value = txt_value.unflatten(-1, (attn.heads, -1))
        
        # ===== QK normalization =====
        # âš¡ å¦‚æœä½¿ç”¨äº†ç¼“å­˜ï¼Œimg éƒ¨åˆ†å·²ç» norm è¿‡äº†
        if not skip_transform:
            if attn.norm_q is not None:
                img_query = attn.norm_q(img_query)
            if attn.norm_k is not None:
                img_key = attn.norm_k(img_key)
        
        if attn.norm_added_q is not None:
            txt_query = attn.norm_added_q(txt_query)
        if attn.norm_added_k is not None:
            txt_key = attn.norm_added_k(txt_key)
        
        # ===== Apply RoPE =====
        # âš¡ å¦‚æœä½¿ç”¨äº†ç¼“å­˜ï¼Œimg éƒ¨åˆ†å·²ç» RoPE è¿‡äº†
        if not skip_transform and image_rotary_emb is not None:
            img_freqs, txt_freqs = image_rotary_emb
            img_query = apply_rotary_emb_qwen(img_query, img_freqs, use_real=False)
            img_key = apply_rotary_emb_qwen(img_key, img_freqs, use_real=False)
        
        if image_rotary_emb is not None:
            _, txt_freqs = image_rotary_emb
            txt_query = apply_rotary_emb_qwen(txt_query, txt_freqs, use_real=False)
            txt_key = apply_rotary_emb_qwen(txt_key, txt_freqs, use_real=False)
        
        # âš¡âš¡âš¡ å…³é”®ä¿®å¤ï¼šåœ¨ reshape/norm/RoPE ä¹‹åç¼“å­˜ï¼
        if should_cache and L_denoise is not None and not should_prune:
            layer_idx = getattr(attn, '_layer_idx', None)
            if layer_idx is not None:
                # ğŸ”¬ å¼€å§‹è®¡æ—¶ clone æ“ä½œ
                if global_pruning_cache.debug_timing:
                    clone_start = torch.cuda.Event(enable_timing=True)
                    clone_end = torch.cuda.Event(enable_timing=True)
                    clone_start.record()
                
                # ç¼“å­˜ç»è¿‡å®Œæ•´å¤„ç†çš„ K, Vï¼ˆreshape + norm + RoPE ä¹‹åï¼‰
                image_k = img_key[:, L_denoise:].clone()
                image_v = img_value[:, L_denoise:].clone()
                
                # ğŸ”¬ ç»“æŸè®¡æ—¶ clone æ“ä½œ
                if global_pruning_cache.debug_timing:
                    clone_end.record()
                    torch.cuda.synchronize()
                    clone_time = clone_start.elapsed_time(clone_end) / 1000.0
                    global_pruning_cache.clone_time += clone_time
                    global_pruning_cache.num_clones += 2  # K å’Œ V
                    global_pruning_cache.cache_write_time += clone_time  # åŠ åˆ°å†™å…¥æ—¶é—´
                
                global_pruning_cache.cache_layer_kv(layer_idx, image_k, image_v, None)
        
        # ===== Concatenate for joint attention =====
        # æ³¨æ„ï¼šimg_query å¯èƒ½æ¯” img_key, img_value çŸ­ï¼ˆpruning æ—¶ï¼‰
        joint_query = torch.cat([txt_query, img_query], dim=1)  # å¯èƒ½ç¼ºå°‘ image query
        joint_key = torch.cat([txt_key, img_key], dim=1)        # å®Œæ•´çš„ key
        joint_value = torch.cat([txt_value, img_value], dim=1)  # å®Œæ•´çš„ value
        
        # ===== Attention mask å¤„ç† =====
        if encoder_hidden_states_mask is not None:
            # ä¸º text tokens åˆ›å»º mask
            batch_size = joint_query.shape[0]
            num_heads = joint_query.shape[2]
            seq_len_q = joint_query.shape[1]
            seq_len_kv = joint_key.shape[1]
            
            # åˆ›å»º attention mask: [B, 1, seq_q, seq_kv]
            attention_mask = torch.zeros(
                (batch_size, 1, seq_len_q, seq_len_kv),
                dtype=joint_query.dtype,
                device=joint_query.device
            )
            
            # Text tokens çš„ maskï¼ˆåª mask text éƒ¨åˆ†ï¼‰
            text_mask = encoder_hidden_states_mask.bool()  # [B, L_txt]
            # æ‰©å±•åˆ°æ‰€æœ‰ query positions
            text_mask_expanded = text_mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, L_txt]
            text_mask_expanded = text_mask_expanded.expand(batch_size, 1, seq_len_q, seq_txt)
            
            # åº”ç”¨ maskï¼ˆå°† padding ä½ç½®è®¾ä¸º -infï¼‰
            attention_mask[:, :, :, :seq_txt] = attention_mask[:, :, :, :seq_txt].masked_fill(
                ~text_mask_expanded, float('-inf')
            )
        
        # ===== Compute joint attention =====
        joint_hidden_states = dispatch_attention_fn(
            joint_query,
            joint_key,
            joint_value,
            attn_mask=attention_mask,
            dropout_p=0.0,
            is_causal=False,
            backend=self._attention_backend,
            parallel_config=self._parallel_config,
        )
        
        # ===== Reshape back =====
        joint_hidden_states = joint_hidden_states.flatten(2, 3)
        joint_hidden_states = joint_hidden_states.to(joint_query.dtype)
        
        # ===== Split attention outputs =====
        txt_attn_output = joint_hidden_states[:, :seq_txt, :]
        img_attn_output = joint_hidden_states[:, seq_txt:, :]  # åªåŒ…å«å»å™ªéƒ¨åˆ†ï¼ˆpruningæ—¶ï¼‰
        
        # ===== Output projections =====
        img_attn_output = attn.to_out[0](img_attn_output)
        if len(attn.to_out) > 1:
            img_attn_output = attn.to_out[1](img_attn_output)
        
        txt_attn_output = attn.to_add_out(txt_attn_output)
        
        # ===== è¿”å›è¾“å‡º =====
        # æ³¨æ„ï¼šåœ¨ pruning æ¨¡å¼ä¸‹ï¼Œimg_attn_output åªåŒ…å«å»å™ªéƒ¨åˆ†
        # Block å±‚é¢ä¼šå¤„ç†æ‹¼æ¥ç¼“å­˜çš„ image tokens
        return img_attn_output, txt_attn_output


class PrunableQwenImageTransformerBlock(nn.Module):
    """
    æ”¯æŒ Token Pruning çš„ Transformer Block
    
    ä» QwenImageTransformerBlock å¤åˆ¶å¹¶ä¿®æ”¹
    """
    
    def __init__(self, original_block, layer_idx):
        super().__init__()
        # å¤åˆ¶åŸå§‹ block çš„æ‰€æœ‰ç»„ä»¶
        self.dim = original_block.dim
        self.num_attention_heads = original_block.num_attention_heads
        self.attention_head_dim = original_block.attention_head_dim
        self.layer_idx = layer_idx
        
        # å¤åˆ¶æ¨¡å—å¼•ç”¨ï¼ˆå…±äº«æƒé‡ï¼‰
        self.img_mod = original_block.img_mod
        self.img_norm1 = original_block.img_norm1
        self.attn = original_block.attn
        self.img_norm2 = original_block.img_norm2
        self.img_mlp = original_block.img_mlp
        
        self.txt_mod = original_block.txt_mod
        self.txt_norm1 = original_block.txt_norm1
        self.txt_norm2 = original_block.txt_norm2
        self.txt_mlp = original_block.txt_mlp
        
        # æ›¿æ¢æ³¨æ„åŠ›å¤„ç†å™¨ä¸ºæ”¯æŒ pruning çš„ç‰ˆæœ¬
        self.attn.processor = PrunableQwenDoubleStreamAttnProcessor()
        
        # â­ å…³é”®ï¼šè®¾ç½® layer_idx åˆ° attnï¼Œä»¥ä¾¿ processor å¯ä»¥è®¿é—®ç¼“å­˜
        self.attn._layer_idx = layer_idx
    
    def _modulate(self, x, mod_params):
        """Apply modulation to input tensor"""
        shift, scale, gate = mod_params.chunk(3, dim=-1)
        return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1), gate.unsqueeze(1)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        encoder_hidden_states_mask: torch.Tensor,
        temb: torch.Tensor,
        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å¸¦ Token Pruning çš„ forward pass
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ pruning
        should_prune = global_pruning_cache.should_prune_current_step()
        L_denoise = global_pruning_cache.denoise_token_length
        
        # ===== è·å–ç¼“å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰=====
        cached_image_k, cached_image_v, cached_image_hidden = None, None, None
        if should_prune and L_denoise is not None:
            cached_image_k, cached_image_v, cached_image_hidden = global_pruning_cache.get_cached_layer_kv(self.layer_idx)
        
        # ===== Modulation parameters =====
        img_mod_params = self.img_mod(temb)
        txt_mod_params = self.txt_mod(temb)
        
        img_mod1, img_mod2 = img_mod_params.chunk(2, dim=-1)
        txt_mod1, txt_mod2 = txt_mod_params.chunk(2, dim=-1)
        
        # ===== Image stream - norm1 + modulation =====
        img_normed = self.img_norm1(hidden_states)
        img_modulated, img_gate1 = self._modulate(img_normed, img_mod1)
        
        # ===== Text stream - norm1 + modulation =====
        txt_normed = self.txt_norm1(encoder_hidden_states)
        txt_modulated, txt_gate1 = self._modulate(txt_normed, txt_mod1)
        
        # ===== Attention computation =====
        joint_attention_kwargs = joint_attention_kwargs or {}
        attn_output = self.attn(
            hidden_states=img_modulated,
            encoder_hidden_states=txt_modulated,
            encoder_hidden_states_mask=encoder_hidden_states_mask,
            image_rotary_emb=image_rotary_emb,
            **joint_attention_kwargs,
        )
        
        img_attn_output, txt_attn_output = attn_output
        
        # ===== å¤„ç† attention è¾“å‡ºï¼ˆè€ƒè™‘ pruningï¼‰=====
        if should_prune:
            # âš ï¸ Pruning æ¨¡å¼ï¼šå¿…é¡»æœ‰ cached_image_hidden
            if cached_image_hidden is None:
                raise RuntimeError("Pruning æ¨¡å¼ä¸‹ç¼ºå°‘ cached_image_hiddenï¼")
            if L_denoise is None:
                raise RuntimeError("Pruning æ¨¡å¼ä¸‹ L_denoise ä¸º Noneï¼")
            
            # âš¡ Pruning æ¨¡å¼ï¼š
            # - img_attn_output åªåŒ…å«å»å™ªéƒ¨åˆ†
            # - image éƒ¨åˆ†ä½¿ç”¨ç¼“å­˜çš„ hidden statesï¼ˆä¸æ›´æ–°ï¼‰
            
            # å»å™ªéƒ¨åˆ†ï¼šåº”ç”¨ attention æ›´æ–°
            denoise_updated = hidden_states[:, :L_denoise] + img_gate1 * img_attn_output
            
            # ç¡®ä¿ç¼“å­˜åœ¨ GPU ä¸Š
            if not cached_image_hidden.is_cuda:
                cached_image_hidden = cached_image_hidden.cuda()
            
            # âš¡ åˆå¹¶ï¼šå»å™ªéƒ¨åˆ†ï¼ˆæ›´æ–°ï¼‰ + å›¾åƒéƒ¨åˆ†ï¼ˆç¼“å­˜ï¼‰
            hidden_states = torch.cat([denoise_updated, cached_image_hidden], dim=1)
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šå®Œæ•´æ›´æ–°
            hidden_states = hidden_states + img_gate1 * img_attn_output
        
        # æ–‡æœ¬æµå§‹ç»ˆæ­£å¸¸æ›´æ–°
        encoder_hidden_states = encoder_hidden_states + txt_gate1 * txt_attn_output
        
        # ===== Image stream - norm2 + MLP =====
        if should_prune:
            # âš ï¸ Pruning æ¨¡å¼ï¼šå¿…é¡»æœ‰ cached_image_hidden
            if cached_image_hidden is None:
                raise RuntimeError("Pruning æ¨¡å¼ä¸‹ç¼ºå°‘ cached_image_hiddenï¼ˆMLP é˜¶æ®µï¼‰ï¼")
            if L_denoise is None:
                raise RuntimeError("Pruning æ¨¡å¼ä¸‹ L_denoise ä¸º Noneï¼ˆMLP é˜¶æ®µï¼‰ï¼")
            
            # âš¡ Pruning æ¨¡å¼ï¼šåªå¯¹å»å™ª tokens è®¡ç®— MLP âš¡
            denoise_hidden = hidden_states[:, :L_denoise]
            denoise_normed2 = self.img_norm2(denoise_hidden)
            denoise_modulated2, denoise_gate2 = self._modulate(denoise_normed2, img_mod2)
            denoise_mlp_output = self.img_mlp(denoise_modulated2)
            denoise_updated = denoise_hidden + denoise_gate2 * denoise_mlp_output
            
            # ç¡®ä¿ç¼“å­˜åœ¨ GPU ä¸Š
            if not cached_image_hidden.is_cuda:
                cached_image_hidden = cached_image_hidden.cuda()
            
            # âš¡ åˆå¹¶ï¼šå»å™ªéƒ¨åˆ†ï¼ˆæ›´æ–°ï¼‰ + å›¾åƒéƒ¨åˆ†ï¼ˆç¼“å­˜ï¼‰
            hidden_states = torch.cat([denoise_updated, cached_image_hidden], dim=1)
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šå®Œæ•´è®¡ç®—
            img_normed2 = self.img_norm2(hidden_states)
            img_modulated2, img_gate2 = self._modulate(img_normed2, img_mod2)
            img_mlp_output = self.img_mlp(img_modulated2)
            hidden_states = hidden_states + img_gate2 * img_mlp_output
        
        # ===== Text stream - norm2 + MLPï¼ˆå§‹ç»ˆå®Œæ•´è®¡ç®—ï¼‰=====
        txt_normed2 = self.txt_norm2(encoder_hidden_states)
        txt_modulated2, txt_gate2 = self._modulate(txt_normed2, txt_mod2)
        txt_mlp_output = self.txt_mlp(txt_modulated2)
        encoder_hidden_states = encoder_hidden_states + txt_gate2 * txt_mlp_output
        
        # ===== Clip for fp16 =====
        if encoder_hidden_states.dtype == torch.float16:
            encoder_hidden_states = encoder_hidden_states.clip(-65504, 65504)
        if hidden_states.dtype == torch.float16:
            hidden_states = hidden_states.clip(-65504, 65504)
        
        # â­ åœ¨ Block ç»“æŸæ—¶ï¼Œæ›´æ–°ç¼“å­˜çš„ hidden statesï¼ˆå¦‚æœéœ€è¦ï¼‰
        # è¿™æ ·ç¼“å­˜çš„æ˜¯ç»è¿‡ Attention + MLP çš„æœ€ç»ˆçŠ¶æ€
        if global_pruning_cache.should_cache_current_step() and L_denoise is not None:
            # ğŸ”¬ å¼€å§‹è®¡æ—¶ clone æ“ä½œ
            if global_pruning_cache.debug_timing:
                clone_start = torch.cuda.Event(enable_timing=True)
                clone_end = torch.cuda.Event(enable_timing=True)
                clone_start.record()
            
            # å¿…é¡» cloneï¼Œå¦åˆ™ä¼šè¢«åç»­æ­¥éª¤ä¿®æ”¹
            image_hidden_final = hidden_states[:, L_denoise:].clone()
            
            # ğŸ”¬ ç»“æŸè®¡æ—¶ clone æ“ä½œ
            if global_pruning_cache.debug_timing:
                clone_end.record()
                torch.cuda.synchronize()
                clone_time = clone_start.elapsed_time(clone_end) / 1000.0
                global_pruning_cache.clone_time += clone_time
                global_pruning_cache.num_clones += 1  # hidden
                global_pruning_cache.cache_write_time += clone_time  # åŠ åˆ°å†™å…¥æ—¶é—´
            
            global_pruning_cache.update_layer_hidden(self.layer_idx, image_hidden_final)
        
        return encoder_hidden_states, hidden_states


def apply_token_pruning_to_transformer(transformer):
    """
    å°† Transformer çš„æ‰€æœ‰ blocks æ›¿æ¢ä¸ºæ”¯æŒ pruning çš„ç‰ˆæœ¬
    
    Args:
        transformer: QwenImageTransformer2DModel å®ä¾‹
    """
    print(f"\nåº”ç”¨ Token Pruning åˆ° Transformer ({len(transformer.transformer_blocks)} å±‚)...")
    
    # æ›¿æ¢æ‰€æœ‰ transformer blocks
    new_blocks = nn.ModuleList()
    for idx, block in enumerate(transformer.transformer_blocks):
        prunable_block = PrunableQwenImageTransformerBlock(block, layer_idx=idx)
        new_blocks.append(prunable_block)
        if (idx + 1) % 10 == 0:
            print(f"  å¤„ç†å±‚ {idx + 1}/{len(transformer.transformer_blocks)}")
    
    transformer.transformer_blocks = new_blocks
    print(f"âœ… å·²æ›¿æ¢ {len(new_blocks)} ä¸ª Transformer Blocks")
    
    return transformer

