"""
å®Œæ•´çš„ Token Pruning æ¨ç†è„šæœ¬
å®ç°ç­–ç•¥: æ­¥éª¤ 1,3 å®Œæ•´è®¡ç®—; æ­¥éª¤ 2,4 ä½¿ç”¨ç¼“å­˜

ä½¿ç”¨æ–¹æ³•:
  python run_with_token_pruning.py -i input.png -p "Your prompt"
  python run_with_token_pruning.py -i input.png -p "Your prompt" --no-pruning  # å¯¹æ¯”åŸºçº¿
"""
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ° pathï¼Œä»¥ä¾¿å¯¼å…¥ pruning_modules
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import math
import argparse
import time
from datetime import datetime
from PIL import Image
from diffusers import FlowMatchEulerDiscreteScheduler

# å¯¼å…¥ pruning æ¨¡å—å’Œè‡ªå®šä¹‰ pipeline
from pruning_modules import (
    global_pruning_cache,
    apply_token_pruning_to_transformer
)
from pruning_pipeline_full import TokenPruningQwenImageEditPipeline


def setup_pipeline_with_pruning(enable_pruning=True):
    """
    è®¾ç½®å¸¦ Token Pruning çš„ Pipeline
    """
    print("=" * 70)
    print("è®¾ç½® Qwen-Image-Edit Lightning Pipeline")
    if enable_pruning:
        print("Token Pruning: âœ… å¯ç”¨ (æ­¥éª¤ 1,3 å®Œæ•´; æ­¥éª¤ 2,4 ç¼“å­˜)")
    else:
        print("Token Pruning: âŒ ç¦ç”¨ (åŸºçº¿å¯¹æ¯”)")
    print("=" * 70)
    
    # 1. é…ç½®è°ƒåº¦å™¨
    print("\n[1/5] é…ç½® FlowMatchEulerDiscreteScheduler...")
    scheduler_config = {
        "base_image_seq_len": 256,
        "base_shift": math.log(3),
        "invert_sigmas": False,
        "max_image_seq_len": 8192,
        "max_shift": math.log(3),
        "num_train_timesteps": 1000,
        "shift": 1.0,
        "shift_terminal": None,
        "stochastic_sampling": False,
        "time_shift_type": "exponential",
        "use_beta_sigmas": False,
        "use_dynamic_shifting": True,
        "use_exponential_sigmas": False,
        "use_karras_sigmas": False,
    }
    scheduler = FlowMatchEulerDiscreteScheduler.from_config(scheduler_config)
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä½¿ç”¨è‡ªå®šä¹‰ Pipelineï¼‰
    print("\n[2/5] åŠ è½½åŸºç¡€æ¨¡å‹: Qwen/Qwen-Image-Edit...")
    pipe = TokenPruningQwenImageEditPipeline.from_pretrained(
        "Qwen/Qwen-Image-Edit",
        scheduler=scheduler,
        torch_dtype=torch.bfloat16
    )
    
    # 3. åŠ è½½ Lightning LoRA
    print("\n[3/5] åŠ è½½ Lightning LoRA æƒé‡...")
    pipe.load_lora_weights(
        "lightx2v/Qwen-Image-Lightning",
        weight_name="Qwen-Image-Edit-2509/Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"
    )
    print("   âœ… LoRA åŠ è½½æˆåŠŸ")
    
    # 4. åº”ç”¨ Token Pruningï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if enable_pruning:
        print("\n[4/5] åº”ç”¨ Token Pruning åˆ° Transformer...")
        apply_token_pruning_to_transformer(pipe.transformer)
        global_pruning_cache.enabled = True
    else:
        print("\n[4/5] è·³è¿‡ Token Pruningï¼ˆåŸºçº¿æ¨¡å¼ï¼‰")
        global_pruning_cache.enabled = False
    
    # 5. ç§»åŠ¨åˆ° CUDA
    print("\n[5/5] ç§»åŠ¨åˆ° CUDA...")
    pipe.to("cuda")
    
    print("\n" + "=" * 70)
    print("âœ… Pipeline è®¾ç½®å®Œæˆï¼")
    print("=" * 70)
    
    return pipe


def run_inference_with_pruning(
    pipe,
    image_path,
    prompt,
    output_dir="outputs_pruning",
    num_steps=4,
    cfg_scale=1.0,
    enable_pruning=True
):
    """
    è¿è¡Œæ¨ç†ï¼ˆå¸¦ Token Pruningï¼‰
    """
    print("\n" + "=" * 70)
    print("å¼€å§‹æ¨ç†")
    print("=" * 70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½å›¾åƒ
    print(f"\n[è¾“å…¥] å›¾åƒ: {image_path}")
    try:
        image = Image.open(image_path).convert("RGB")
        print(f"       å°ºå¯¸: {image.size}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½å›¾åƒ - {e}")
        return None, None, None
    
    print(f"[è¾“å…¥] Prompt: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")
    print(f"[å‚æ•°] æ¨ç†æ­¥æ•°: {num_steps}")
    print(f"[å‚æ•°] CFG Scale: {cfg_scale}")
    print(f"[å‚æ•°] Token Pruning: {'å¯ç”¨' if enable_pruning else 'ç¦ç”¨ (åŸºçº¿å¯¹æ¯”)'}")
    
    # å‡†å¤‡æ¨ç†å‚æ•°
    inference_params = {
        "image": image,
        "prompt": prompt,
        "negative_prompt": " ",
        "num_inference_steps": num_steps,
        "true_cfg_scale": cfg_scale,
        "generator": torch.manual_seed(42),
    }
    
    # é‡ç½® pruning çŠ¶æ€
    global_pruning_cache.clear_caches()
    global_pruning_cache.current_step = 0
    
    # æ‰§è¡Œæ¨ç†
    print("\n" + "-" * 70)
    print(f"{'æ¨ç†è¿‡ç¨‹ (Token Pruning)' if enable_pruning else 'æ¨ç†è¿‡ç¨‹ (Baseline)'}:")
    print("-" * 70)
    
    # â±ï¸ å¼€å§‹è®¡æ—¶
    print("\nâ±ï¸  è®¡æ—¶å¼€å§‹...")
    inference_start = time.time()
    
    try:
        # ä½¿ç”¨è‡ªå®šä¹‰ Pipeline çš„ __call__ æ–¹æ³•
        # Token é•¿åº¦ä¿¡æ¯ä¼šåœ¨å†…éƒ¨è‡ªåŠ¨è®¾ç½®
        with torch.inference_mode():
            output = pipe(**inference_params)
            output_image = output.images[0]
    
    except Exception as e:
        print(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None
    
    # â±ï¸ ç»“æŸè®¡æ—¶
    inference_time = time.time() - inference_start
    print(f"\nâ±ï¸  æ¨ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f} ç§’")
    
    # ğŸ”¬ æ‰“å°ç¼“å­˜æ“ä½œçš„è¯¦ç»†ç»Ÿè®¡
    if enable_pruning:
        global_pruning_cache.print_timing_stats()
    
    # ä¿å­˜ç»“æœ
    print("\n" + "-" * 70)
    print("ä¿å­˜ç»“æœ:")
    print("-" * 70)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    suffix = "pruning" if enable_pruning else "baseline"
    output_filename = f"output_{suffix}_{timestamp}.png"
    output_path = os.path.join(output_dir, output_filename)
    
    output_image.save(output_path)
    print(f"âœ… æ–‡ä»¶: {output_path}")
    
    # ä¿å­˜æœ€æ–°ç‰ˆæœ¬
    latest_path = os.path.join(output_dir, f"latest_{suffix}.png")
    output_image.save(latest_path)
    print(f"   æœ€æ–°: {latest_path}")
    
    # æ—¶é—´ç»Ÿè®¡
    print(f"\n" + "=" * 70)
    print(f"â±ï¸  æ€§èƒ½ç»Ÿè®¡:")
    print("=" * 70)
    print(f"  æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’")
    print(f"  æ¨¡å¼: {'Token Pruning' if enable_pruning else 'Baseline (æ— ä¼˜åŒ–)'}")
    
    return output_image, output_path, inference_time


def main():
    """
    ä¸»ç¨‹åº
    """
    parser = argparse.ArgumentParser(
        description='Qwen-Image-Edit Lightning + Token Pruning å®Œæ•´å®ç°',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¯ç”¨ Token Pruning
  python run_with_token_pruning.py -i input.png -p "Make it purple"
  
  # ç¦ç”¨ Pruningï¼ˆå¯¹æ¯”åŸºçº¿ï¼‰
  python run_with_token_pruning.py -i input.png -p "Make it purple" --no-pruning
  
  # å¯¹æ¯”å®éªŒ
  python run_with_token_pruning.py -p "Your prompt" --no-pruning  # è¿è¡ŒåŸºçº¿
  python run_with_token_pruning.py -p "Your prompt"              # è¿è¡Œ pruning
  # å¯¹æ¯” outputs_pruning/ ä¸­çš„ä¸¤ä¸ªè¾“å‡º
        """
    )
    
    parser.add_argument('--input', '-i', type=str, default='input.png',
                        help='è¾“å…¥å›¾ç‰‡è·¯å¾„ (é»˜è®¤: input.png)')
    parser.add_argument('--prompt', '-p', type=str,
                        default='Change the rabbit\'s color to purple',
                        help='ç¼–è¾‘æŒ‡ä»¤')
    parser.add_argument('--output_dir', '-o', type=str, default='outputs_pruning',
                        help='è¾“å‡ºç›®å½• (é»˜è®¤: outputs_pruning)')
    parser.add_argument('--steps', '-s', type=int, default=4,
                        help='æ¨ç†æ­¥æ•° (é»˜è®¤: 4)')
    parser.add_argument('--cfg', '-c', type=float, default=1.0,
                        help='CFG Scale (é»˜è®¤: 1.0)')
    parser.add_argument('--no-pruning', action='store_true',
                        help='ç¦ç”¨ Token Pruningï¼ˆç”¨äºå¯¹æ¯”åŸºçº¿ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
    
    # è®¾ç½® Pipeline
    pipe = setup_pipeline_with_pruning(enable_pruning=not args.no_pruning)
    
    # è¿è¡Œæ¨ç†
    output_image, output_path, inference_time = run_inference_with_pruning(
        pipe,
        image_path=args.input,
        prompt=args.prompt,
        output_dir=args.output_dir,
        num_steps=args.steps,
        cfg_scale=args.cfg,
        enable_pruning=not args.no_pruning
    )
    
    if output_path:
        print("\n" + "=" * 70)
        print("âœ… å®éªŒå®Œæˆï¼")
        print("=" * 70)
        
        mode_name = "Token Pruning" if not args.no_pruning else "Baseline"
        print(f"\nğŸ“Š å®éªŒç»“æœ:")
        print(f"  æ¨¡å¼: {mode_name}")
        print(f"  æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’")
        print(f"  è¾“å‡ºæ–‡ä»¶: {output_path}")
        
        if not args.no_pruning:
            print("\nğŸ’¡ æç¤º: è¿è¡ŒåŸºçº¿å¯¹æ¯”ä»¥è¯„ä¼°åŠ é€Ÿæ•ˆæœ:")
            print(f"  python run_with_token_pruning.py \\")
            print(f"      -i {args.input} \\")
            print(f"      -p \"{args.prompt[:50]}...\" \\")
            print(f"      --no-pruning")
            print(f"\n  ç„¶åå¯¹æ¯”:")
            print(f"    outputs_pruning/latest_pruning.png  â† Token Pruning")
            print(f"    outputs_pruning/latest_baseline.png â† Baseline")
        else:
            print("\nğŸ’¡ æç¤º: è¿è¡Œ Token Pruning ç‰ˆæœ¬:")
            print(f"  python run_with_token_pruning.py \\")
            print(f"      -i {args.input} \\")
            print(f"      -p \"{args.prompt[:50]}...\"")
            print(f"\n  æŸ¥çœ‹åŠ é€Ÿæ•ˆæœå’Œè´¨é‡å¯¹æ¯”")


if __name__ == "__main__":
    main()

