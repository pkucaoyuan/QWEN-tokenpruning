"""
æµ‹è¯•è‡ªå®šä¹‰ Processor çš„çœŸå®å¼€é”€
é€šè¿‡å¯¹æ¯”åŸå§‹ Pipeline å’Œè‡ªå®šä¹‰ Processorï¼ˆä¸å¯ç”¨ pruningï¼‰çš„æ€§èƒ½
"""
import torch
import time
from diffusers import DiffusionPipeline
from PIL import Image
import sys

def test_baseline():
    """æµ‹è¯•åŸå§‹ Baselineï¼ˆä¸ä½¿ç”¨ä»»ä½•è‡ªå®šä¹‰ä»£ç ï¼‰"""
    print("=" * 70)
    print("æµ‹è¯• 1: Baseline Pipelineï¼ˆåŸå§‹å®ç°ï¼‰")
    print("=" * 70)
    
    pipe = DiffusionPipeline.from_pretrained(
        "Qwen/Qwen-Image-Edit",
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
    ).to("cuda")
    
    # åŠ è½½ Lightning LoRA
    pipe.load_lora_weights("./models", weight_name="Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors")
    
    # åŠ è½½å›¾åƒ
    input_image = Image.open("input.png").convert("RGB")
    
    # é¢„çƒ­
    print("\né¢„çƒ­...")
    _ = pipe(
        prompt="test",
        image=input_image,
        height=1080,
        width=1620,
        num_inference_steps=4,
        guidance_scale=1.0,
    ).images[0]
    
    # æ­£å¼æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
    print("\næ­£å¼æµ‹è¯•ï¼ˆ3æ¬¡ï¼‰...")
    times = []
    for i in range(3):
        torch.cuda.synchronize()
        start = time.time()
        
        _ = pipe(
            prompt="Convert the male person to female",
            image=input_image,
            height=1080,
            width=1620,
            num_inference_steps=4,
            guidance_scale=1.0,
        ).images[0]
        
        torch.cuda.synchronize()
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"  è¿è¡Œ {i+1}: {elapsed:.2f}s")
    
    avg_time = sum(times) / len(times)
    print(f"\nâœ… Baseline å¹³å‡æ—¶é—´: {avg_time:.2f}s")
    
    return avg_time


def test_custom_processor_without_pruning():
    """æµ‹è¯•è‡ªå®šä¹‰ Processorï¼ˆä¸å¯ç”¨ pruningï¼‰"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: è‡ªå®šä¹‰ Processorï¼ˆPruning ç¦ç”¨ï¼‰")
    print("=" * 70)
    
    # å¯¼å…¥è‡ªå®šä¹‰ pipeline
    sys.path.insert(0, '.')
    from pruning_pipeline_full import TokenPruningQwenImageEditPipeline
    from pruning_modules import global_pruning_cache
    
    pipe = TokenPruningQwenImageEditPipeline.from_pretrained(
        "Qwen/Qwen-Image-Edit",
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
    ).to("cuda")
    
    # åŠ è½½ Lightning LoRA
    pipe.load_lora_weights("./models", weight_name="Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors")
    
    # âš ï¸ å…³é”®ï¼šä¸å¯ç”¨ pruning
    print("\nâš ï¸  Pruning: ç¦ç”¨ï¼ˆæµ‹è¯•çº¯ Processor å¼€é”€ï¼‰")
    
    # åŠ è½½å›¾åƒ
    input_image = Image.open("input.png").convert("RGB")
    
    # é¢„çƒ­
    print("\né¢„çƒ­...")
    _ = pipe(
        prompt="test",
        image=input_image,
        height=1080,
        width=1620,
        num_inference_steps=4,
        guidance_scale=1.0,
        enable_pruning=False,  # ç¦ç”¨ pruning
    ).images[0]
    
    # æ­£å¼æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
    print("\næ­£å¼æµ‹è¯•ï¼ˆ3æ¬¡ï¼‰...")
    times = []
    for i in range(3):
        torch.cuda.synchronize()
        start = time.time()
        
        _ = pipe(
            prompt="Convert the male person to female",
            image=input_image,
            height=1080,
            width=1620,
            num_inference_steps=4,
            guidance_scale=1.0,
            enable_pruning=False,  # ç¦ç”¨ pruning
        ).images[0]
        
        torch.cuda.synchronize()
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"  è¿è¡Œ {i+1}: {elapsed:.2f}s")
    
    avg_time = sum(times) / len(times)
    print(f"\nâœ… è‡ªå®šä¹‰ Processorï¼ˆæ—  Pruningï¼‰å¹³å‡æ—¶é—´: {avg_time:.2f}s")
    
    return avg_time


def test_custom_processor_with_pruning():
    """æµ‹è¯•è‡ªå®šä¹‰ Processorï¼ˆå¯ç”¨ pruningï¼‰"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: è‡ªå®šä¹‰ Processorï¼ˆPruning å¯ç”¨ï¼‰")
    print("=" * 70)
    
    # å¯¼å…¥è‡ªå®šä¹‰ pipeline
    sys.path.insert(0, '.')
    from pruning_pipeline_full import TokenPruningQwenImageEditPipeline
    from pruning_modules import global_pruning_cache
    
    pipe = TokenPruningQwenImageEditPipeline.from_pretrained(
        "Qwen/Qwen-Image-Edit",
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
    ).to("cuda")
    
    # åŠ è½½ Lightning LoRA
    pipe.load_lora_weights("./models", weight_name="Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors")
    
    print("\nâœ… Pruning: å¯ç”¨")
    
    # åŠ è½½å›¾åƒ
    input_image = Image.open("input.png").convert("RGB")
    
    # é¢„çƒ­
    print("\né¢„çƒ­...")
    _ = pipe(
        prompt="test",
        image=input_image,
        height=1080,
        width=1620,
        num_inference_steps=4,
        guidance_scale=1.0,
        enable_pruning=True,  # å¯ç”¨ pruning
    ).images[0]
    
    # æ­£å¼æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
    print("\næ­£å¼æµ‹è¯•ï¼ˆ3æ¬¡ï¼‰...")
    times = []
    for i in range(3):
        torch.cuda.synchronize()
        start = time.time()
        
        _ = pipe(
            prompt="Convert the male person to female",
            image=input_image,
            height=1080,
            width=1620,
            num_inference_steps=4,
            guidance_scale=1.0,
            enable_pruning=True,  # å¯ç”¨ pruning
        ).images[0]
        
        torch.cuda.synchronize()
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"  è¿è¡Œ {i+1}: {elapsed:.2f}s")
    
    avg_time = sum(times) / len(times)
    print(f"\nâœ… è‡ªå®šä¹‰ Processorï¼ˆå¯ç”¨ Pruningï¼‰å¹³å‡æ—¶é—´: {avg_time:.2f}s")
    
    return avg_time


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ğŸ”¬ ç²¾ç¡®æµ‹é‡è‡ªå®šä¹‰ Processor çš„å¼€é”€")
    print("=" * 70)
    
    # æµ‹è¯• 1: Baseline
    baseline_time = test_baseline()
    
    # æµ‹è¯• 2: è‡ªå®šä¹‰ Processorï¼ˆä¸å¯ç”¨ pruningï¼‰
    custom_no_pruning_time = test_custom_processor_without_pruning()
    
    # æµ‹è¯• 3: è‡ªå®šä¹‰ Processorï¼ˆå¯ç”¨ pruningï¼‰
    custom_with_pruning_time = test_custom_processor_with_pruning()
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š ç»“æœæ±‡æ€»")
    print("=" * 70)
    print(f"1. Baseline:                    {baseline_time:.2f}s")
    print(f"2. è‡ªå®šä¹‰ Processorï¼ˆæ—  Pruningï¼‰: {custom_no_pruning_time:.2f}s")
    print(f"3. è‡ªå®šä¹‰ Processorï¼ˆå¯ç”¨ Pruningï¼‰: {custom_with_pruning_time:.2f}s")
    print()
    print(f"è‡ªå®šä¹‰ Processor æœ¬èº«çš„å¼€é”€:    {custom_no_pruning_time - baseline_time:+.2f}s ({(custom_no_pruning_time/baseline_time-1)*100:+.1f}%)")
    print(f"Pruning çš„å‡€æ•ˆæœ:              {custom_with_pruning_time - custom_no_pruning_time:+.2f}s ({(custom_with_pruning_time/custom_no_pruning_time-1)*100:+.1f}%)")
    print(f"æ€»ä½“æ•ˆæœï¼ˆvs Baselineï¼‰:        {custom_with_pruning_time - baseline_time:+.2f}s ({(custom_with_pruning_time/baseline_time-1)*100:+.1f}%)")
    print("=" * 70)

